{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from openai import OpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "#insert your Open AI API key here.\n",
    "open_api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key = open_api_key)\n",
    "embedding_model = OpenAIEmbeddings(openai_api_key = open_api_key)\n",
    "metadata_id = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing Chroma DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "placeholder = \"\"\n",
    "doc = Document(page_content=placeholder, metadata={\"_id_\": f\"{metadata_id}\"})\n",
    "db = Chroma.from_documents([doc], embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VectorDB Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_text(text):\n",
    "    global metadata_id\n",
    "    current_metadata = metadata_id\n",
    "    metadata = {\"id\":f\"{current_metadata}\"}\n",
    "    metadata_id +=1\n",
    "\n",
    "    doc = Document(page_content=text, metadata=metadata)\n",
    "    db.add_documents([doc])\n",
    "\n",
    "def query_text(query, num_suggestions = 4):\n",
    "    global db\n",
    "    search_result = db.similarity_search(query=query, k=num_suggestions)\n",
    "    return search_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Response Generation LLM Calls (3 Types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "STORAGE_SYSTEM_MESSAGE = {\n",
    "\"role\":\"system\",\n",
    "\"content\":\"\"\"\n",
    "You are a helpful Journal Assistant that generates an acknowledgement notification about information that you have been told to remember or store.\n",
    "You will be given an input and you need to generate a suitable acknowledgement notification output for this input. Your output has to have a reference\n",
    "to the input text, but don't replicate the input text as it is.\n",
    "\"\"\"\n",
    "}\n",
    "\n",
    "storage_few_shot_examples = [{\"role\":\"user\",\"content\":\"Remind me to buy eggs when I'm at the supermarket next.\"},\n",
    "                     {\"role\":\"assistant\",\"content\":\"Sure, I'll remind you about buying eggs when you visit the supermarket next.\"},\n",
    "                     {\"role\":\"user\",\"content\":\"I need to be reminded to do my math assignment and book my flight tickets when I open my laptop next.\"},\n",
    "                     {\"role\":\"assistant\",\"content\":\"The next time you open your laptop, I'll be sure to remind you about your assignment and flight tickets!\"}]\n",
    "storage_messages = [STORAGE_SYSTEM_MESSAGE]\n",
    "storage_messages.extend(storage_few_shot_examples)\n",
    "\n",
    "def generate_llm_storage_response(text):\n",
    "    global client\n",
    "    global storage_messages\n",
    "\n",
    "    messages = storage_messages.copy()\n",
    "    messages.append({\"role\":\"user\", \"content\":text})\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo-0125\",  \n",
    "    messages=messages\n",
    "    )\n",
    "\n",
    "    return response    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "IRRELEVANT_SYSTEM_MESSAGE = {\n",
    "\"role\":\"system\",\n",
    "\"content\":\"\"\"\n",
    "You are a helpful Journal Assistant that generates an error message when given an input that is unrelated to your purpose as a journal app.\n",
    "You will be given an input and you need to generate a suitable error notification output for this input. Your output has to have a reference to the fact\n",
    "that you are just a journal application, the domain of the input, and how the domain of the input is greatly different from the expertise of a journal\n",
    "application. \n",
    "\"\"\"\n",
    "}\n",
    "\n",
    "irrelevant_few_shot_examples = [{\"role\":\"user\",\"content\":\"What's 2+2?\"},\n",
    "                     {\"role\":\"assistant\",\"content\":\"I'm just a journal application, and cannot perform mathematical calculations.\"},\n",
    "                     {\"role\":\"user\",\"content\":\"Who is the president of India?\"},\n",
    "                     {\"role\":\"assistant\",\"content\":\"I do not have extensive general knowledge, because I'm just a journal application.\"}]\n",
    "irrelevant_messages = [IRRELEVANT_SYSTEM_MESSAGE]\n",
    "irrelevant_messages.extend(irrelevant_few_shot_examples)\n",
    "\n",
    "def generate_llm_irrelevant_response(text):\n",
    "    global client\n",
    "    global irrelevant_messages\n",
    "\n",
    "    messages = irrelevant_messages.copy()\n",
    "    messages.append({\"role\":\"user\", \"content\":text})\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo-0125\",  \n",
    "    messages=messages\n",
    "    )\n",
    "\n",
    "    return response "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "RETRIEVAL_SYSTEM_MESSAGE = {\n",
    "\"role\":\"system\",\n",
    "\"content\":\"\"\"\n",
    "You are a helpful Journal Assistant that generates a description o\n",
    "You will be given multiple inputs-\n",
    "1. The first will be a Query.\n",
    "2. Some of the following inputs will be sentences related to the query- they will be the answer to the query. \n",
    "   Some of the other inputs will not be related to the query-they will not be the answer to the query.\n",
    "\n",
    "You need to generate an output which combines all the input sentences which are related to the input query and presents them as an \n",
    "answer to the input query. You must not include the unrelated input sentences in your answer to the input query.\n",
    "You could have a large number of such input sentences. You need to judge which input sentences are related and should be included in your \n",
    "answer to the input query, and which input sentences are not related and shouldn't be included in your answer.\n",
    "\n",
    "Here are some examples:\n",
    "-<inputQuery>I'm at the supermarket now. What should I buy?</inputQuery>\n",
    " <inputSentence>Remind me to buy eggs when I'm at the supermarket next.</inputSentence> \n",
    " <inputSentence>I need to buy a birthday card for my mom from the supermarket.</inputSentence>\n",
    " <inputSentence>Remind me to get a haircut tomorrow.</inputSentence>\n",
    "  Response- \"You should buy eggs, and a birthday card for your mom from the supermarket.\"\n",
    "\n",
    "-<inputQuery>I just opened my laptop, what should I do?</inputQuery>\n",
    " <inputSentence>Remind me to email the job recruiter at Google, when I'm on my laptop next.</inputSentence>\n",
    " <inputSentence>Remind me about that meeting I have with John tomorrow.</inputSentence>\n",
    " <inputSentence>I need to work on my Math Assignment and book flight tickets on my laptop</inputSentence>\n",
    "  Response- \"You should email the job recruiter at Google, work on your Math Assignment and book flight tickets using your laptop.\"\n",
    "\"\"\"\n",
    "}\n",
    "\n",
    "def generate_llm_retrieval_response(query, search_results):\n",
    "    global RETRIEVAL_SYSTEM_MESSAGE\n",
    "    global client\n",
    "\n",
    "    query_message ={\"role\": \"user\", \"content\": query}\n",
    "    search_messages = [{\"role\": \"user\", \"content\": search_result.page_content} for search_result in search_results]\n",
    "\n",
    "    messages = [RETRIEVAL_SYSTEM_MESSAGE]\n",
    "    messages.append(query_message)\n",
    "\n",
    "    for search_message in search_messages:\n",
    "        messages.append(search_message)\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo-0125\",  \n",
    "    messages=messages)\n",
    "\n",
    "    return response \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions Using Classifier Output to Make Appropriate LLM Call, and Generate Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_information(text):\n",
    "    store_text(text)\n",
    "    response = generate_llm_storage_response(text)\n",
    "    return response\n",
    "\n",
    "def retrieve_information(query):\n",
    "    search_results = query_text(query)\n",
    "    response = generate_llm_retrieval_response(query, search_results)\n",
    "    return response\n",
    "\n",
    "def handle_irrelevant(text):\n",
    "    response = generate_llm_irrelevant_response(text)\n",
    "    return response\n",
    "\n",
    "# Dispatcher function\n",
    "def call_function(args,name):\n",
    "    if name == \"store\":\n",
    "        return store_information(args['text'])\n",
    "    elif name == \"retrieve\":\n",
    "        return retrieve_information(args['text'])\n",
    "    elif name == \"irrelevant\":\n",
    "        return handle_irrelevant(args['text'])\n",
    "    else:\n",
    "        raise ValueError(\"Unknown function\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classifier LLM (outputting a JSON with Appropriate Function Arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_CLASSIFIER_SYSTEM_MESSAGE = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"\"\"\n",
    "You are a helpful assistant that classifies user inputs into three categories: storage, retrieval, and irrelevant.\n",
    "\n",
    "You will store information like grocery lists, reminders, incidents, future tasks, daily entries, dreams, things that you need to do in the future etc.\n",
    "\n",
    "If you get a query that asks for information that may have been stored in a journal app, you should say that the query is for retrieval.\n",
    "If the query is not asking general information, but asking for tasks, past incidents, future plans, etc., you should say that the query is for retrieval.\n",
    "\n",
    "If you feel the query is irrelevant for a journal app, you can say that the query is irrelevant. They will be about topics unrelated to a journal app, \n",
    "typically containing information that you have never stored or retrieved.                                          \n",
    "                                                 \n",
    "Read the query carefully and think whether a journal app should store the query, retrieve information from the query, or if the query is irrelevant.\n",
    "\n",
    "Here are some examples:\n",
    "\n",
    "1. Storage:\n",
    "- \"Remind me to buy eggs when I'm at the supermarket next.\"\n",
    "- \"Note that I have a meeting with John at 3 PM tomorrow.\"\n",
    "- \"I was stuck in traffic today for a very long time.\"\n",
    "\n",
    "2. Retrieval:\n",
    "- \"I'm at the supermarket now. What should I buy?\"\n",
    "- \"What meetings do I have scheduled for tomorrow?\"\n",
    "- \"What's on my to-do list today?\"\n",
    "\n",
    "3. Irrelevant:\n",
    "- \"What's 2+2?\"\n",
    "- \"Tell me a joke.\"\n",
    "- \"What is the capital of Delhi\"\n",
    "\"\"\"\n",
    "}\n",
    "\n",
    "\n",
    "def generate_classifier_response(prompt):\n",
    "    tools = [\n",
    "        {\n",
    "            \"type\":\"function\",\n",
    "            \"function\":{\n",
    "            \"name\": \"store\",\n",
    "            \"description\": \"Stores the input text for future retrieval.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"text\": {\"type\": \"string\", \"description\": \"The actual text to store\"}\n",
    "                },\n",
    "                \"required\": [\"text\"]\n",
    "            }\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"type\":\"function\",\n",
    "            \"function\":{\n",
    "            \"name\": \"retrieve\",\n",
    "            \"description\": \"Retrieves previously stored information based on the input text.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"text\": {\"type\": \"string\", \"description\": \"The input text to retrieve information for\"}\n",
    "                },\n",
    "                \"required\": [\"text\"]\n",
    "            }\n",
    "            }\n",
    "        },\n",
    "        {   \n",
    "            \"type\":\"function\",\n",
    "            \"function\":{\n",
    "            \"name\": \"irrelevant\",\n",
    "            \"description\": \"Handles inputs that are irrelevant to storage or retrieval. If you feel it is not store or retrieve, you can say that the input is irrelevant.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"text\": {\"type\": \"string\", \"description\": \"The irrelevant input text\"}\n",
    "                },\n",
    "                \"required\": [\"text\"]\n",
    "            }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    tool_call = \"required\"\n",
    "\n",
    "    global PROMPT_CLASSIFIER_SYSTEM_MESSAGE\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo-0125\",  \n",
    "        messages=[\n",
    "            PROMPT_CLASSIFIER_SYSTEM_MESSAGE,\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        tools=tools,\n",
    "        tool_choice=tool_call    \n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.tool_calls[0].function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating Suitable Response Depending on Classifier Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_overall_model_response(prompt):\n",
    "    classifier_response = generate_classifier_response(prompt)\n",
    "    \n",
    "    if classifier_response:\n",
    "        arguments = classifier_response.arguments\n",
    "        name = classifier_response.name\n",
    "        arguments = json.loads(arguments)\n",
    "        result = call_function(arguments, name)\n",
    "        final_response_text = result.choices[0].message.content\n",
    "        \n",
    "        return final_response_text\n",
    "    else:\n",
    "        error_message = \"No function call was made by the model.\"\n",
    "        display_message = \"Sorry, I'm unable to answer your request at this moment.\"\n",
    "        return display_message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing End to End with a Few Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'll make sure to remind you about the eggs when you're at the supermarket next.\n"
     ]
    }
   ],
   "source": [
    "info = \"Remind me to buy eggs when I'm at the supermarket next.\"\n",
    "info_resp = generate_overall_model_response(info)\n",
    "print(info_resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I've noted down your meeting with John at 3 PM tomorrow.\n"
     ]
    }
   ],
   "source": [
    "info2 = \"Note that I have a meeting with John at 3 PM tomorrow.\"\n",
    "info2_resp = generate_overall_model_response(info2)\n",
    "print(info2_resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noted, I will remind you to work on your math assignment and book flight tickets using your laptop.\n"
     ]
    }
   ],
   "source": [
    "info3 = \"Remind me to work on my math assignment and book my flight tickets, using my laptop.\"\n",
    "info3_resp = generate_overall_model_response(info3)\n",
    "print(info3_resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On your grocery list, you should buy eggs when at the supermarket next.\n"
     ]
    }
   ],
   "source": [
    "question = \"I'm at the supermarket now. What should I buy?\"\n",
    "ques_response = generate_overall_model_response(question)\n",
    "print(ques_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your to-do list for today includes working on your math assignment, booking flight tickets using your laptop, and buying eggs when you're at the supermarket next. Additionally, you have a meeting scheduled with John at 3 PM tomorrow.\n"
     ]
    }
   ],
   "source": [
    "prompt_1 = \"What is on my to-do list today?\"\n",
    "response_1 = generate_overall_model_response(prompt_1)\n",
    "print(response_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noted, I will remind you about getting a haircut tomorrow.\n"
     ]
    }
   ],
   "source": [
    "prompt_2 = \"Remind me to get a haircut tomorrow.\"\n",
    "response_2 = generate_overall_model_response(prompt_2)\n",
    "print(response_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I do not have extensive general knowledge, because I'm just a journal application.\n"
     ]
    }
   ],
   "source": [
    "prompt_3 = \"Who is the president of India?\"\n",
    "response_3 = generate_overall_model_response(prompt_3)\n",
    "print(response_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm just a journal application, and do not provide general knowledge or geographical information.\n"
     ]
    }
   ],
   "source": [
    "prompt_4 = \"What is the capital of England?\"\n",
    "response_4 = generate_overall_model_response(prompt_4)\n",
    "print(response_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm just a journal application, and cannot perform mathematical calculations.\n"
     ]
    }
   ],
   "source": [
    "prompt_5 = \"What's 2+2?\"\n",
    "response_5 = generate_overall_model_response(prompt_5)\n",
    "print(response_5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmi_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
